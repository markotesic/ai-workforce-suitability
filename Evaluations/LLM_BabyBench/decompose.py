# evaluators/decompose.py

from copy import deepcopy

from typing import Any, Optional, Tuple, List, Dict


from Evaluations.LLM_BabyBench.babyaibot import BabyAIBot
from Evaluations.LLM_BabyBench.utils import parse_str_subgoals, instantiate_subgoals

MAX_HELP = 30 # from running 1000 seeds on all unbugged environments, the maximum help needed was 27


class DecomposeEvaluator():
    """
    Evaluator for the BabyAI-Subgoal task.
    (Env description, Start state, Mission) -> Subgoal Sequence
    """
    def evaluate(self, env, llm_output: str, **kwargs) -> Dict[str, Any]:
        """
        To assess the quality of subgoals generated by the LLM, we follow these steps:

        1 - Subgoal Instantiation: We instantiate the subgoals produced by the LLM.

        2 - Bot Execution: We pass these subgoals to a modified version of the bot that can accept a predefined list of subgoals as input.

        3 - Bot Operation: We run the bot in the environment, allowing it to add additional subgoals if the initial list is insufficient for completing the mission.

        4 - Metric Evaluation: We evaluate performance using the following metrics:

            + Success (âˆž-Perfect Accuracy): Does the bot ultimately reach the goal? That is, do the LLM-generated subgoals include (at minimum) the correct decomposition of the overall mission?

            + 0-Perfect Accuracy: Does the bot reach the goal without needing to add any additional subgoals?

            + 1-Perfect Accuracy: Does the bot reach the goal after adding at most one extra subgoal?

            + 3-Perfect Accuracy: Does the bot reach the goal after adding at most three extra subgoals?
        """

        local_env = deepcopy(env)
        bot = BabyAIBot(local_env, [])
        list_of_subgoals = list(llm_output.split("\n"))
        list_of_subgoals = parse_str_subgoals(list_of_subgoals)
        subgoals = instantiate_subgoals(bot, list_of_subgoals)
        bot.reset(subgoals_plan=subgoals)

        done = False
        truncated = False
        last_action = None
        while not done and not truncated and len(bot.stack) > 0:
            action = bot.replan(last_action)
            obs, reward, done, truncated, info = local_env.step(action)
            last_action = action
            local_env.render()

        help_counter = bot.return_help_count()

        local_env_validate = deepcopy(env)
        bot_validate = BabyAIBot(local_env_validate,[])
        bot_validate.provide_initial_subgoals()
        done_validate = False
        truncated_validate = False
        last_action = None
        while not done_validate and not truncated_validate and len(bot_validate.stack) > 0:
            action = bot_validate.replan(last_action)
            obs, reward, done_validate, truncated_validate, info = local_env_validate.step(action)
            last_action = action
            local_env_validate.render()
        max_help = bot_validate.return_help_count()

        return {
            "CR": float(done),
            "PR": float(done and help_counter == 0),
            "ACI" : max(0,(max_help - help_counter)/max_help) if done else 0.
        }
    